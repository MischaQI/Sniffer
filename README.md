## CVPR24-SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection <img src="logo.png" width="50" height="50">
[Peng Qi](https://pengqi.site/), [Zehong Yan](https://scholar.google.co.uk/citations?hl=en&user=GkUGt0cAAAAJ), [Wynne Hsu](https://www.comp.nus.edu.sg/~whsu/), [Mong Li Lee](https://www.comp.nus.edu.sg/~leeml/)

<a href="https://pengqi.site/Sniffer/"><img src="https://img.shields.io/badge/Project_Page-Online-b31b1b"></a> 
<a href="https://huggingface.co/MischaQI/SNIFFER"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Model_Card-Huggingface-orange"></a> 
<a href="https://arxiv.org/abs/2403.03170"><img src="https://img.shields.io/badge/Paper-Arxiv_2403.03170-yellow.svg"></a>
<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Qi_SNIFFER_Multimodal_Large_Language_Model_for_Explainable_Out-of-Context_Misinformation_Detection_CVPR_2024_paper.html"><img src="https://img.shields.io/badge/Paper-CVPR2024-green.svg"></a>
<a href="https://opensource.org/licenses/Apache-2.0"><img src="https://img.shields.io/badge/License-Apache%202.0-4a9bb0.svg"></a>
<!-- dataset -->

## Abstract
Misinformation is a prevalent societal issue due to its potential high risks. Out-Of-Context (OOC) misinformation where authentic images are repurposed with false text is one of the easiest and most effective ways to mislead audiences. Current methods focus on assessing image-text consistency but lack convincing explanations for their judgments which are essential for debunking misinformation. While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation they still lack sophistication in understanding and discovering the subtle cross-modal differences. In this paper we introduce SNIFFER a novel multimodal large language model specifically engineered for OOC misinformation detection and explanation. SNIFFER employs two-stage instruction tuning on InstructBLIP. The first stage refines the model's concept alignment of generic objects with news-domain entities and the second stage leverages OOC-specific instruction data generated by language-only GPT-4 to fine-tune the model's discriminatory powers. Enhanced by external tools and retrieval SNIFFER not only detects inconsistencies between text and image but also utilizes external knowledge for contextual verification. Our experiments show that SNIFFER surpasses the original MLLM by over 40% and outperforms state-of-the-art methods in detection accuracy. SNIFFER also provides accurate and persuasive explanations as validated by quantitative and human evaluations. 

<video width='510' src="https://pengqi.site/Sniffer/assets/SNIFFER-demo-short.mov"></video>


<!-- ## News
- [08/02/2024] Test Benchmark is Released [Here](benchmark/readme.md)!
- [20/12/2023] The implementation based on [threestudio](https://github.com/threestudio-project/threestudio) is released! Please checkout [this branch](https://github.com/HeliosZhao/Animate124/tree/threestudio).
- [05/12/2023] Code Released! -->

## TODO
- [x] Release inferecne demo code
- [ ] Release training and evaluation code


## Getting Started
### Installation
```bash
$ conda create -n lavis python=3.10
$ conda activate lavis
$ pip install torch==2.1.2 torchvision==0.16.2 
$ pip install -r requirements.txt
```


## How to run

### Inference Demo
A whole pipeline that includes internal checking based on the mllm module, external checking and combined reasoning based on the llm module (could be arbitrarily replaced with other better llms). 
```bash
$ python demo_inference.py
```

## Citation
If you make use of our work, please cite our paper.
```bibtex
@InProceedings{Qi_2024_CVPR,
    author    = {Qi, Peng and Yan, Zehong and Hsu, Wynne and Lee, Mong Li},
    title     = {SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {13052-13062}
}
```

## Acknowledgements
We build our code on top of the [LAVIS](https://github.com/salesforce/LAVIS). We sincerely thank to LAVIS team for the amazing work and well-structured code. 